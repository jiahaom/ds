<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on</title><link>https://jiahaom.github.io/ds/</link><description>Recent content in Machine Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jiahaom.github.io/ds/index.xml" rel="self" type="application/rss+xml"/><item><title>Classification</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Classification/</guid><description>Know your metrics.
use datasets to obtain/estimate the [[#^68e85f|Posterior Probabilities]] instead of using. Type [[content/Machine_Learning/Linear_Classifier]]: the simplest boundary [[content/Machine_Learning/Logistic_Model#^d37bff|Logistic Classifier]]: Approximated [[content/Machine_Learning/The_Bayes_Classifier#^68e85f|Posterior Probabilities]] [[K-Nearest]]: Approximated [[content/Machine_Learning/The_Bayes_Classifier#^68e85f|Posterior Probabilities]] [[content/Machine_Learning/The_Bayes_Classifier]]: True [[content/Machine_Learning/The_Bayes_Classifier#^68e85f|Posterior Probabilities]] Evaluation [[content/Machine_Learning/Classification_Quality_Metric]]
Shape of boundaries: Logistic regression and LDA build linear boundaries, QDA quadratic boundaries. kNN does not impose any particular shape. Stability: For a small number of samples, logistic regression can be very unstable, whereas LDA approaches produce stable solutions.</description></item><item><title>Classification_Quality_Metric</title><link>https://jiahaom.github.io/ds/Machine_Learning/Classification_Quality_Metric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Classification_Quality_Metric/</guid><description>Accuracy [[content/Machine_Learning/Regression_Quality_Metric#^e51d1f]]
[[content/Machine_Learning/KNN]] and [[content/Machine_Learning/Logistic_Model#^bbc977|Logistic Regression]] don&amp;rsquo;t have! Error Rate (content/Machine_Learning/Regression_Quality_Metric#^9d4269)
[[content/Machine_Learning/KNN]] and [[content/Machine_Learning/Logistic_Model#^bbc977|Logistic Regression]] don&amp;rsquo;t have! [[content/Machine_Learning/The_Bayes_Classifier#^a80775|A Bayesian Extension]] if 1 patient in 1000 people, we say all people are healthy, the accuracy is 99.9%.
Confusion Matrix to see how classifier treat each class individually Counting ![[Screenshot 2021-11-06 at 15.10.46.png]] 3 red samples are misclassified as blue Rate when working with imbalanced datasets, counts might be misleading !</description></item><item><title>Configuration</title><link>https://jiahaom.github.io/ds/notes/config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/config/</guid><description>Configuration Quartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you&amp;rsquo;d like to get.
The majority of configuration can be be found under data/config.yaml. An annotated example configuration is shown below.
1 2 3 4 5 6 7 8 9 10 name:Your name here!# Shows in the footerenableToc:true# Whether to show a Table of Contentsdescription:Page description to show to search enginespage_title:Quartz Example Page# Default Page Titlelinks:# Links to show in footer- link_name:Twitterlink:https://twitter.</description></item><item><title>Deploying Quartz to the Web</title><link>https://jiahaom.github.io/ds/notes/hosting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/hosting/</guid><description>GitHub Pages Quartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! You can head to &amp;lt;YOUR-GITHUB-USERNAME.github.io/quartz to see it live.
Enable GitHub Actions By default, GitHub disables workflows from running automatically on Forked Repostories. Head to the &amp;lsquo;Actions&amp;rsquo; tab of your forked repository and Enable Workflows to setup deploying your Quartz site!
Enable GitHub Actions</description></item><item><title>Editing Content in Quartz</title><link>https://jiahaom.github.io/ds/notes/editing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/editing/</guid><description>Editing Quartz runs on top of Hugo so all notes are written in Markdown .
Obsidian I strongly recommend using Obsidian as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of my local files.
🔗 How to link your Obsidian Vault
Of course, all the files are in Markdown so you could just use your favourite text editor of choice.</description></item><item><title>Equivalent_numerical_representation</title><link>https://jiahaom.github.io/ds/Machine_Learning/Equivalent_numerical_representation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Equivalent_numerical_representation/</guid><description>different units: miles vs km incommensurable: weight vs height different dynamic ranges: cm vs km outlier sensitive: an outlier 10 times larger than the 2nd will squeeze min-max to [0, 0.1] quality guarantee on pipeline instead of these models. Min-max normalization $z = \frac {x - min(x)}{max(x) - min(x)}$ 0 &amp;lt;= z &amp;lt;= 1 min(x) and max(x) are used during test and deployment Standardization $z = \frac {x - \mu}{\sigma}$ mean = 0, unit standard deviation.</description></item><item><title>Ignoring Notes</title><link>https://jiahaom.github.io/ds/notes/ignore-notes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/ignore-notes/</guid><description>Quartz Ignore Edit ignoreFiles in config.toml to include paths you&amp;rsquo;d like to exclude from being rendered.
1 2 3 4 5 6 ... ignoreFiles = [ &amp;#34;/content/templates/*&amp;#34;, &amp;#34;/content/private/*&amp;#34;, &amp;#34;&amp;lt;your path here&amp;gt;&amp;#34; ] ignoreFiles supports the use of Regular Expressions (RegEx) so you can ignore patterns as well (e.g. ignoring all .pngs by doing \\.png$). To ignore a specific file, you can also add the tag draft: true to the frontmatter of a note.</description></item><item><title>Introduction</title><link>https://jiahaom.github.io/ds/Machine_Learning/Introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Introduction/</guid><description>Machine Learning tools for modeling and understanding [[content/Machine_Learning/Introduction#^a091d7|data]] &amp;ndash;&amp;gt; [[content/Machine_Learning/Introduction#^d8965c|knowledge]] Data an observation or measurement or item ^a091d7
neutral and raw: doesn&amp;rsquo;t show anything
Attributes/ Identify Animal (ID): Wild mouse, Rabbit &amp;hellip; Body mass (g): 22, 4000 &amp;hellip; HR (bpm): 480, 250 &amp;hellip; Science [[content/Machine_Learning/Introduction#^240154|evaluating]] our knowledge &amp;raquo;&amp;gt; sophisticated instrumentation
evaluation data with accepted knowledge ^240154 Knowledge Proposition: T/F (statement, law) Narrative: (business) Model: y=10x+3 (Math or Computer) ^d8965c AI act or thing rationally might use ML algorithms or use statistics or computation ^482c57</description></item><item><title>KNN</title><link>https://jiahaom.github.io/ds/Machine_Learning/KNN/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/KNN/</guid><description> Non-parametric, instance-based approaches
doesn&amp;rsquo;t have accuracy and error rate! Nearest Neighbours compare the whole nearest sample assigned the label of the closest (most similar) training sample K Nearest Neighbours can be easily implemented in multi-class compare the K nearest sample assigned the label of the closest (most similar) training sample</description></item><item><title>Linear_Classifier</title><link>https://jiahaom.github.io/ds/Machine_Learning/Linear_Classifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Linear_Classifier/</guid><description> use straight lines/ planes/ hyperplanes
Boundaries linear equation: $w^Tx=0$ extended vector / predictors: $x=[1, x_1,x_2,&amp;hellip;]^T$ coefficients vector: $w=[w_0, w_1,w_2&amp;hellip;]$ e.g. 2D $x_2 = -\frac{w_1}{w_2}x_1-\frac{w_0}{w_2}$ Definition $w^Tx&amp;gt;0$: one side of the boundary $w^Tx&amp;lt;0$: other side of the boundary</description></item><item><title>Linear_Regression</title><link>https://jiahaom.github.io/ds/Machine_Learning/Linear_Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Linear_Regression/</guid><description>The best linear model for training data: [[content/Machine_Learning/Regression_Quality_Metric#^d91d7e|The least squares]] Simple Linear Regression $f(x) = w_0+w_1x$
$\hat y = f(x_i)= w^TX = w_0+w_1x_i$ ^b14eb7
one predictor: x $X=[1,x_i]^T$ one label: y use a dataset to tune two parameters to achieve the highest quality $w = [w_0, w_1]$ $w_0: intercept$ $w_1: slope$ Multiple Linear Regression $\hat y = f(x_i) = w^Tx_i = w0+w_1x_{i,1} +&amp;hellip;+w_Kx_{i,K}$ $w=[w_o,w_1,&amp;hellip;,w_k]$ predictors: $x_i=[1,x_{i,1},x_{i,2}, &amp;hellip;,x_{i,K}]^T$ k-th predictor of the i-th sample e.</description></item><item><title>Logistic_Model</title><link>https://jiahaom.github.io/ds/Machine_Learning/Logistic_Model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Logistic_Model/</guid><description>Classifier ^d37bff
$p(distance)=\frac{e^d}{1+e^d} = \frac{1}{1+e^{−d}}$
$p(0) = 0.5$ $As\ d → ∞,\ p(d) → 1$ $As\ d → −∞,\ p(d) → 0$ if we set the distance between boundary and sample: $d = w^Tx_i$
d: label logistic function: $p(d) = \frac {e^{w^Tx_i}}{1+e^{w^Tx_i}}$ Regression ^bbc977 $L= \prod_{y=red} (1 − p(x_i)) \prod_{y=blue} p(x_i) = 0$ [[content/Machine_Learning/Regression_Quality_Metric]] doesn&amp;rsquo;t use accuracy and error rate!</description></item><item><title>Machine_Learning</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Machine_Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Machine_Learning/</guid><description>Taxonomy ![[F1ECE30F-8DD2-464A-8AB2-DA25F4025743.jpeg]] ![[734CF636-E861-4899-BFCA-DBFF70E0776D.jpeg]]
[[content/Machine_Learning/Structure/Supervised_Learning]] [[content/Machine_Learning/Structure/Unsupervised_Learning]] [[content/Machine_Learning/Methodology]]</description></item><item><title>Methodology</title><link>https://jiahaom.github.io/ds/Machine_Learning/Methodology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Methodology/</guid><description>The Value of Knowledge ![[985DCBB1-0DD1-4752-9504-81CCE2A02E6A.jpeg]]
The Truth [[Sampling#^41ffae|Dataset]] != [[Sampling#^d1148e|Population]] no [[Sampling#^d1148e|Population]] in the ML no True Performance no absolute no best random: different datasets/ performance ^ccf020 1. [[content/Machine_Learning/Training]] fit a model to dataset 2. [[content/Machine_Learning/Optimisation]] 3. [[content/Machine_Learning/Validation]] find the best mode 4. [[content/Machine_Learning/Testing]] verify the model Don&amp;rsquo;t let appearances fool you!
kg vs cm y = 3 + 100Xa + 20Xb linearly separable Pipeline !</description></item><item><title>Neural_Network</title><link>https://jiahaom.github.io/ds/Machine_Learning/Neural_Network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Neural_Network/</guid><description> loosely mimmic neurons a universal machine neuroscience mathematics Architectures: a family of models![[Screenshot 2021-11-17 at 10.28.27.png]] input: predictors output: label weights: parameters can be tuned/ trained</description></item><item><title>Obsidian Vault Integration</title><link>https://jiahaom.github.io/ds/notes/obsidian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/obsidian/</guid><description>Setup Obsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.
New Vault If you don&amp;rsquo;t have an existing Vault, download Obsidian and create a new Vault in the /content folder that you created and cloned during the setup .
Linking an existing Vault The easiest way to use an existing Vault is to copy all of our files (directory and hierarchies intact) into the /content folder.</description></item><item><title>Optimisation</title><link>https://jiahaom.github.io/ds/Machine_Learning/Optimisation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Optimisation/</guid><description>find the best model if we have:
a collection of candidate models quality metric ideal description of the target population Gradient Descent ^e433dc update iteratively / parameter tuning $w_{new} = w_{old}-\epsilon ∇E(w_{old})$ $w: model$ $\epsilon: learning\ rate/step\ size$ $∇E(w_{old}): slope$ large: overshoot small: slow initial model $w$ is crucial usually chosen randomly stopping strategy Gradient Descent will not reach the optimal model Number of iterations.</description></item><item><title>Preview Changes</title><link>https://jiahaom.github.io/ds/notes/preview-changes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/preview-changes/</guid><description>If you&amp;rsquo;d like to preview what your Quartz site looks like before deploying it to the internet, here&amp;rsquo;s exactly how to do that!
Install hugo-obsidian This step will generate the list of backlinks for Hugo to parse. Ensure you have Go (&amp;gt;= 1.16) installed.
1 2 3 4 5 6 7 8 # Install and link `hugo-obsidian` locally $ go install github.com/jackyzha0/hugo-obsidian # Navigate to your local Quartz folder $ cd &amp;lt;location-of-your-local-quartz&amp;gt; # Scrape all links in your Quartz folder and generate info for Quartz $ hugo-obsidian -input=content -output=data -index=true Afterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated!</description></item><item><title>Quartz Philosophy</title><link>https://jiahaom.github.io/ds/notes/philosophy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/philosophy/</guid><description>“[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming
Why Quartz? Hosting a public digital garden isn&amp;rsquo;t easy. There are an overwhelming number of tutorials, resources, and guides for tools like Notion , Roam , and Obsidian , yet none of them have super easy to use free tools to publish that garden to the world.</description></item><item><title>Regression</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Regression/</guid><description>Mathematical ![[Pasted image 20211006161933.png]]
N: the number of samples i: index x: predictor y: (continuous) true label dataset: {(xi,yi) ∶ 1 ≤ i ≤ N}, (xi,yi) is sample i f(⋅): model yˆi = f(xi): predicted label ei = yi − yˆi: prediction error 1. [[content/Machine_Learning/Regression_Quality_Metric]] 2. Model generate multiple shapes by tuning parameters.
![[Pasted image 20211006194951.png]] Priors: Type of model (linear, polynomial, etc). Data: Labelled samples, predictors and true (continuous) label.</description></item><item><title>Regression_Quality_Metric</title><link>https://jiahaom.github.io/ds/Machine_Learning/Regression_Quality_Metric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Regression_Quality_Metric/</guid><description>think it before build it embrace the error no precise input data no perfect solution therefore, represent this discrepancy as: $y = \hat{y}+e = f(x)+e$ find association between attributes instead of causation build relationship instead of causal models Error ^9d4269 $E = \frac {N(incorrect\ samples)}{N(samples)}$
Estimate mean squared error/ MSE ^a03fa6 ![[Pasted image 20211006162907.png]]
find the minimum MSE: !</description></item><item><title>Setup</title><link>https://jiahaom.github.io/ds/notes/setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/setup/</guid><description>Making your own Quartz Setting up Quartz requires a basic understanding of git. If you are unfamiliar, this resource is a great place to start!
Forking A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.
Navigate to the GitHub repository for the Quartz project:
📁 Quartz Repository Then, Fork the repository into your own GitHub account.</description></item><item><title>Showcase</title><link>https://jiahaom.github.io/ds/moc/showcase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/moc/showcase/</guid><description>Want to see what Quartz can do? Here are some cool community gardens :)
Quartz Documentation (this site!) Strengthening Online Social Bonds: Research Garden Jacky Zhao&amp;rsquo;s Garden Anson Yu&amp;rsquo;s Garden Shihyu&amp;rsquo;s PKM Chloe&amp;rsquo;s Garden SlRvb&amp;rsquo;s Site Course notes for Information Technology Advanced Theory If you want to see your own on here, submit a Pull Request adding yourself to this file !</description></item><item><title>Supervised_Learning</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Supervised_Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Supervised_Learning/</guid><description> estimate/ predict the missing value/ label/ attribute
[[content/Machine_Learning/Structure/Classification]]: T/F [[content/Machine_Learning/Structure/Regression]]: y = 2x+3</description></item><item><title>Testing</title><link>https://jiahaom.github.io/ds/Machine_Learning/Testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Testing/</guid><description>Performance best model: with the highest deployment performance evaluate the performance quality metric: to quantify ^f67765 data: to assess designed before build avoid data-traps (like confirmation bias) focus on test task &amp;raquo;&amp;gt; spilt the data Assessing test dataset: [[Sampling#^41ffae|a subset of data]], randomly extracted test deployment performance: estimation of the true performance comparing: The Infinite Monkey Theorem
lower MSE !</description></item><item><title>The_Bayes_Classifier</title><link>https://jiahaom.github.io/ds/Machine_Learning/The_Bayes_Classifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/The_Bayes_Classifier/</guid><description>The Bayes Classifier ^95fc31 a classifier uses the True [[#^68e85f|Posterior Probabilities]] use odds ratio([[#^68e85f|Posterior Probabilities]] or [[#^555d72|Class Priors]]+[[#^3a3076|Class Densities]]) to achieves the highest accuracy by comparing: $\frac {P(y=red|x)}{P(y=blue|x)} = \frac {P(x|y=red)P(y=red)}{P(x|y=blue)P(y=blue)}$ ^839888 if &amp;gt;1 ==&amp;gt; y = red if &amp;lt; 1 ==&amp;gt; y = blue a ideal classifier: we never know $\frac {P(y=red|x)}{P(y=blue|x)}$ ![[Screenshot 2021-11-06 at 10.25.09.png]]
Class Priors ^555d72 assume 70 % of the population y=red, 30% y=blue.</description></item><item><title>Training</title><link>https://jiahaom.github.io/ds/Machine_Learning/Training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Training/</guid><description>What we have a family of candidate models: e.g. linear models [[content/Machine_Learning/Regression_Quality_Metric]]: e.g. the error training data: ![[Screenshot 2021-11-15 at 10.04.34.png]] therefore, we can get only empirical error surface ^10ee42 ![[Screenshot 2021-11-14 at 15.23.55.png]] instead of true error surface. ![[Screenshot 2021-11-14 at 15.23.01.png]] [[content/Machine_Learning/Optimisation#^607e71]]
What can we do</description></item><item><title>Transform</title><link>https://jiahaom.github.io/ds/Machine_Learning/Transform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Transform/</guid><description>change the way represent samples
one of the most important: many ML models = a transformation + a simple model extract features from each pic as training data. ![[Screenshot 2021-11-02 at 19.30.59.png]] if we extract 4 values, we need fewer training pics. can be either hand-crafted or trained. after training remain fixed. if we know how to transform: learn the model which operates on the destination space if we don&amp;rsquo;t know: learn it by selecting or tuning; Kernel method Same Number of Dimensions Linear PCA provide an importance score for each component / to each dimension of the destination space 1 2 3 4 flowchart LR A --&amp;gt; C A(10D) --&amp;gt;|PCA| B(10D) B(10D) --&amp;gt;|score| C(5D) score: ^3ea892 rank the attributes defining the destination space remove the least important feature [[#^04faa5|feature selection]] Rotation PCA + Rotation: !</description></item><item><title>Troubleshooting and FAQ</title><link>https://jiahaom.github.io/ds/notes/troubleshooting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/notes/troubleshooting/</guid><description>Common Pitfalls Can I publish only a subset of my pages? Yes! Quartz makes selective publishing really easy. Heres a guide on excluding pages from being published .
Can I host this myself and not on GitHub Pages? Yes! All built files can be found under /public in the master branch. More details under hosting .
Do I need a website already? No! Setting up Quartz means you set up a site too :)</description></item><item><title>Unsupervised_Learning</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Unsupervised_Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Unsupervised_Learning/</guid><description>find the underlying structure to identify anomalies/ outlier - Structure Analysis: - Cluster analysis: groups of data points - Component analysis: directions of interest - Density Estimation: describe the distribution of samples in the attribute space</description></item><item><title>Validation</title><link>https://jiahaom.github.io/ds/Machine_Learning/Validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Validation/</guid><description>models work together &amp;ndash;&amp;gt; select the best family
a family of models with random subsets of the data (learn different things) different models with random subsets of attributes (like [[#^648b6a|wrapping]]) different family of models altogether Validation set approach ![[Screenshot 2021-11-16 at 10.09.01.png]] result: error LOOCV: Leave-one-out cross-validation ![[Screenshot 2021-11-16 at 10.10.35.png]] result: error.avg() k-fold cross-validation LOOCV is a special case of k-fold cross-validation, where k = N .</description></item></channel></rss>
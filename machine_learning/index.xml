<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine_Learnings on</title><link>https://jiahaom.github.io/ds/machine_learning/</link><description>Recent content in Machine_Learnings on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://jiahaom.github.io/ds/machine_learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Classification</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Classification/</guid><description>Know your metrics.
use datasets to obtain/estimate the (#^68e85f|Posterior Probabilities.md) instead of using. Type Linear_Classifier : the simplest boundary Logistic Classifier : Approximated Posterior Probabilities K-Nearest.md: Approximated Posterior Probabilities The_Bayes_Classifier : True Posterior Probabilities Evaluation Classification_Quality_Metric Shape of boundaries: Logistic regression and LDA build linear boundaries, QDA quadratic boundaries. kNN does not impose any particular shape.</description></item><item><title>Classification_Quality_Metric</title><link>https://jiahaom.github.io/ds/Machine_Learning/Classification_Quality_Metric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Classification_Quality_Metric/</guid><description>Accuracy [[content/Machine_Learning/Regression_Quality_Metric#^e51d1f]]
[[content/Machine_Learning/KNN]] and [[content/Machine_Learning/Logistic_Model#^bbc977|Logistic Regression]] don&amp;rsquo;t have! Error Rate (content/Machine_Learning/Regression_Quality_Metric#^9d4269)
[[content/Machine_Learning/KNN]] and [[content/Machine_Learning/Logistic_Model#^bbc977|Logistic Regression]] don&amp;rsquo;t have! [[content/Machine_Learning/The_Bayes_Classifier#^a80775|A Bayesian Extension]] if 1 patient in 1000 people, we say all people are healthy, the accuracy is 99.9%.
Confusion Matrix to see how classifier treat each class individually Counting ![[Screenshot 2021-11-06 at 15.10.46.png]] 3 red samples are misclassified as blue Rate when working with imbalanced datasets, counts might be misleading !</description></item><item><title>Equivalent_numerical_representation</title><link>https://jiahaom.github.io/ds/Machine_Learning/Equivalent_numerical_representation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Equivalent_numerical_representation/</guid><description>different units: miles vs km incommensurable: weight vs height different dynamic ranges: cm vs km outlier sensitive: an outlier 10 times larger than the 2nd will squeeze min-max to [0, 0.1] quality guarantee on pipeline instead of these models. Min-max normalization $z = \frac {x - min(x)}{max(x) - min(x)}$ 0 &amp;lt;= z &amp;lt;= 1 min(x) and max(x) are used during test and deployment Standardization $z = \frac {x - \mu}{\sigma}$ mean = 0, unit standard deviation.</description></item><item><title>Introduction</title><link>https://jiahaom.github.io/ds/Machine_Learning/Introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Introduction/</guid><description>Machine Learning tools for modeling and understanding [[content/Machine_Learning/Introduction#^a091d7|data]] &amp;ndash;&amp;gt; [[content/Machine_Learning/Introduction#^d8965c|knowledge]] Data an observation or measurement or item ^a091d7
neutral and raw: doesn&amp;rsquo;t show anything
Attributes/ Identify Animal (ID): Wild mouse, Rabbit &amp;hellip; Body mass (g): 22, 4000 &amp;hellip; HR (bpm): 480, 250 &amp;hellip; Science [[content/Machine_Learning/Introduction#^240154|evaluating]] our knowledge &amp;raquo;&amp;gt; sophisticated instrumentation
evaluation data with accepted knowledge ^240154 Knowledge Proposition: T/F (statement, law) Narrative: (business) Model: y=10x+3 (Math or Computer) ^d8965c AI act or thing rationally might use ML algorithms or use statistics or computation ^482c57</description></item><item><title>KNN</title><link>https://jiahaom.github.io/ds/Machine_Learning/KNN/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/KNN/</guid><description> Non-parametric, instance-based approaches
doesn&amp;rsquo;t have accuracy and error rate! Nearest Neighbours compare the whole nearest sample assigned the label of the closest (most similar) training sample K Nearest Neighbours can be easily implemented in multi-class compare the K nearest sample assigned the label of the closest (most similar) training sample</description></item><item><title>Linear_Classifier</title><link>https://jiahaom.github.io/ds/Machine_Learning/Linear_Classifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Linear_Classifier/</guid><description> use straight lines/ planes/ hyperplanes
Boundaries linear equation: $w^Tx=0$ extended vector / predictors: $x=[1, x_1,x_2,&amp;hellip;]^T$ coefficients vector: $w=[w_0, w_1,w_2&amp;hellip;]$ e.g. 2D $x_2 = -\frac{w_1}{w_2}x_1-\frac{w_0}{w_2}$ Definition $w^Tx&amp;gt;0$: one side of the boundary $w^Tx&amp;lt;0$: other side of the boundary</description></item><item><title>Linear_Regression</title><link>https://jiahaom.github.io/ds/Machine_Learning/Linear_Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Linear_Regression/</guid><description>The best linear model for training data: [[content/Machine_Learning/Regression_Quality_Metric#^d91d7e|The least squares]] Simple Linear Regression $f(x) = w_0+w_1x$
$\hat y = f(x_i)= w^TX = w_0+w_1x_i$ ^b14eb7
one predictor: x $X=[1,x_i]^T$ one label: y use a dataset to tune two parameters to achieve the highest quality $w = [w_0, w_1]$ $w_0: intercept$ $w_1: slope$ Multiple Linear Regression $\hat y = f(x_i) = w^Tx_i = w0+w_1x_{i,1} +&amp;hellip;+w_Kx_{i,K}$ $w=[w_o,w_1,&amp;hellip;,w_k]$ predictors: $x_i=[1,x_{i,1},x_{i,2}, &amp;hellip;,x_{i,K}]^T$ k-th predictor of the i-th sample e.</description></item><item><title>Logistic_Model</title><link>https://jiahaom.github.io/ds/Machine_Learning/Logistic_Model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Logistic_Model/</guid><description>Classifier ^d37bff
$p(distance)=\frac{e^d}{1+e^d} = \frac{1}{1+e^{−d}}$
$p(0) = 0.5$ $As\ d → ∞,\ p(d) → 1$ $As\ d → −∞,\ p(d) → 0$ if we set the distance between boundary and sample: $d = w^Tx_i$
d: label logistic function: $p(d) = \frac {e^{w^Tx_i}}{1+e^{w^Tx_i}}$ Regression ^bbc977 $L= \prod_{y=red} (1 − p(x_i)) \prod_{y=blue} p(x_i) = 0$ [[content/Machine_Learning/Regression_Quality_Metric]] doesn&amp;rsquo;t use accuracy and error rate!</description></item><item><title>Machine_Learning</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Machine_Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Machine_Learning/</guid><description>Taxonomy ![[F1ECE30F-8DD2-464A-8AB2-DA25F4025743.jpeg]] ![[734CF636-E861-4899-BFCA-DBFF70E0776D.jpeg]]
[[content/Machine_Learning/Structure/Supervised_Learning]] [[content/Machine_Learning/Structure/Unsupervised_Learning]] [[content/Machine_Learning/Methodology]]</description></item><item><title>Methodology</title><link>https://jiahaom.github.io/ds/Machine_Learning/Methodology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Methodology/</guid><description>The Value of Knowledge ![[985DCBB1-0DD1-4752-9504-81CCE2A02E6A.jpeg]]
The Truth [[Sampling#^41ffae|Dataset]] != [[Sampling#^d1148e|Population]] no [[Sampling#^d1148e|Population]] in the ML no True Performance no absolute no best random: different datasets/ performance ^ccf020 1. [[content/Machine_Learning/Training]] fit a model to dataset 2. [[content/Machine_Learning/Optimisation]] 3. [[content/Machine_Learning/Validation]] find the best mode 4. [[content/Machine_Learning/Testing]] verify the model Don&amp;rsquo;t let appearances fool you!
kg vs cm y = 3 + 100Xa + 20Xb linearly separable Pipeline !</description></item><item><title>Neural_Network</title><link>https://jiahaom.github.io/ds/Machine_Learning/Neural_Network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Neural_Network/</guid><description> loosely mimmic neurons a universal machine neuroscience mathematics Architectures: a family of models![[Screenshot 2021-11-17 at 10.28.27.png]] input: predictors output: label weights: parameters can be tuned/ trained</description></item><item><title>Optimisation</title><link>https://jiahaom.github.io/ds/Machine_Learning/Optimisation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Optimisation/</guid><description>find the best model if we have:
a collection of candidate models quality metric ideal description of the target population Gradient Descent ^e433dc update iteratively / parameter tuning $w_{new} = w_{old}-\epsilon ∇E(w_{old})$ $w: model$ $\epsilon: learning\ rate/step\ size$ $∇E(w_{old}): slope$ large: overshoot small: slow initial model $w$ is crucial usually chosen randomly stopping strategy Gradient Descent will not reach the optimal model Number of iterations.</description></item><item><title>Regression</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Regression/</guid><description>Mathematical ![[Pasted image 20211006161933.png]]
N: the number of samples i: index x: predictor y: (continuous) true label dataset: {(xi,yi) ∶ 1 ≤ i ≤ N}, (xi,yi) is sample i f(⋅): model yˆi = f(xi): predicted label ei = yi − yˆi: prediction error 1. [[content/Machine_Learning/Regression_Quality_Metric]] 2. Model generate multiple shapes by tuning parameters.
![[Pasted image 20211006194951.png]] Priors: Type of model (linear, polynomial, etc). Data: Labelled samples, predictors and true (continuous) label.</description></item><item><title>Regression_Quality_Metric</title><link>https://jiahaom.github.io/ds/Machine_Learning/Regression_Quality_Metric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Regression_Quality_Metric/</guid><description>think it before build it embrace the error no precise input data no perfect solution therefore, represent this discrepancy as: $y = \hat{y}+e = f(x)+e$ find association between attributes instead of causation build relationship instead of causal models Error ^9d4269 $E = \frac {N(incorrect\ samples)}{N(samples)}$
Estimate mean squared error/ MSE ^a03fa6 ![[Pasted image 20211006162907.png]]
find the minimum MSE: !</description></item><item><title>Supervised_Learning</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Supervised_Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Supervised_Learning/</guid><description> estimate/ predict the missing value/ label/ attribute
[[content/Machine_Learning/Structure/Classification]]: T/F [[content/Machine_Learning/Structure/Regression]]: y = 2x+3</description></item><item><title>Testing</title><link>https://jiahaom.github.io/ds/Machine_Learning/Testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Testing/</guid><description>Performance best model: with the highest deployment performance evaluate the performance quality metric: to quantify ^f67765 data: to assess designed before build avoid data-traps (like confirmation bias) focus on test task &amp;raquo;&amp;gt; spilt the data Assessing test dataset: [[Sampling#^41ffae|a subset of data]], randomly extracted test deployment performance: estimation of the true performance comparing: The Infinite Monkey Theorem
lower MSE !</description></item><item><title>The_Bayes_Classifier</title><link>https://jiahaom.github.io/ds/Machine_Learning/The_Bayes_Classifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/The_Bayes_Classifier/</guid><description>The Bayes Classifier ^95fc31 a classifier uses the True [[#^68e85f|Posterior Probabilities]] use odds ratio([[#^68e85f|Posterior Probabilities]] or [[#^555d72|Class Priors]]+[[#^3a3076|Class Densities]]) to achieves the highest accuracy by comparing: $\frac {P(y=red|x)}{P(y=blue|x)} = \frac {P(x|y=red)P(y=red)}{P(x|y=blue)P(y=blue)}$ ^839888 if &amp;gt;1 ==&amp;gt; y = red if &amp;lt; 1 ==&amp;gt; y = blue a ideal classifier: we never know $\frac {P(y=red|x)}{P(y=blue|x)}$ ![[Screenshot 2021-11-06 at 10.25.09.png]]
Class Priors ^555d72 assume 70 % of the population y=red, 30% y=blue.</description></item><item><title>Training</title><link>https://jiahaom.github.io/ds/Machine_Learning/Training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Training/</guid><description>What we have a family of candidate models: e.g. linear models [[content/Machine_Learning/Regression_Quality_Metric]]: e.g. the error training data: ![[Screenshot 2021-11-15 at 10.04.34.png]] therefore, we can get only empirical error surface ^10ee42 ![[Screenshot 2021-11-14 at 15.23.55.png]] instead of true error surface. ![[Screenshot 2021-11-14 at 15.23.01.png]] [[content/Machine_Learning/Optimisation#^607e71]]
What can we do</description></item><item><title>Transform</title><link>https://jiahaom.github.io/ds/Machine_Learning/Transform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Transform/</guid><description>change the way represent samples
one of the most important: many ML models = a transformation + a simple model extract features from each pic as training data. ![[Screenshot 2021-11-02 at 19.30.59.png]] if we extract 4 values, we need fewer training pics. can be either hand-crafted or trained. after training remain fixed. if we know how to transform: learn the model which operates on the destination space if we don&amp;rsquo;t know: learn it by selecting or tuning; Kernel method Same Number of Dimensions Linear PCA provide an importance score for each component / to each dimension of the destination space 1 2 3 4 flowchart LR A --&amp;gt; C A(10D) --&amp;gt;|PCA| B(10D) B(10D) --&amp;gt;|score| C(5D) score: ^3ea892 rank the attributes defining the destination space remove the least important feature [[#^04faa5|feature selection]] Rotation PCA + Rotation: !</description></item><item><title>Unsupervised_Learning</title><link>https://jiahaom.github.io/ds/Machine_Learning/Structure/Unsupervised_Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Structure/Unsupervised_Learning/</guid><description>find the underlying structure to identify anomalies/ outlier - Structure Analysis: - Cluster analysis: groups of data points - Component analysis: directions of interest - Density Estimation: describe the distribution of samples in the attribute space</description></item><item><title>Validation</title><link>https://jiahaom.github.io/ds/Machine_Learning/Validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jiahaom.github.io/ds/Machine_Learning/Validation/</guid><description>models work together &amp;ndash;&amp;gt; select the best family
a family of models with random subsets of the data (learn different things) different models with random subsets of attributes (like [[#^648b6a|wrapping]]) different family of models altogether Validation set approach ![[Screenshot 2021-11-16 at 10.09.01.png]] result: error LOOCV: Leave-one-out cross-validation ![[Screenshot 2021-11-16 at 10.10.35.png]] result: error.avg() k-fold cross-validation LOOCV is a special case of k-fold cross-validation, where k = N .</description></item></channel></rss>